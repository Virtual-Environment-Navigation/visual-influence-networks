{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute leadership measures\n",
    "Calculate leadership measures for each pedestrian in individual networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run *2_get_leadership.py* on Oscar\n",
    "CI values are too computationally heavy, so run this script on supercomputer for each trial/segment (we used [Oscar](https://docs.ccv.brown.edu/oscar))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compile the trial files\n",
    "\n",
    "Output:\n",
    "* `../data/pickle/sayles_leadership.p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compile trial 7\n",
    "import pickle\n",
    "\n",
    "# ===== CHANGE THIS ========================================\n",
    "ntwk_window_sizes = [0.5]   # , 1\n",
    "prune_type = 'pruned'       # unpruned or pruned\n",
    "# ==========================================================\n",
    "\n",
    "measure_types = ['NI', 'NBI', 'CI', 'CBI']\n",
    "trial = 7\n",
    "\n",
    "for ntwk_window_size in ntwk_window_sizes:\n",
    "    ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "    weights = pickle.load( open(f'../data/pickle/sayles_weights_pruned_{ntwk_window_size_ms}ms.p', 'rb') )\n",
    "    segs = weights[trial].keys()\n",
    "\n",
    "    leadership = {}\n",
    "    for measure_type in measure_types:\n",
    "        leadership[measure_type] = {}\n",
    "        leadership[measure_type+'rank'] = {}\n",
    "\n",
    "    for seg in segs:\n",
    "        filename_seg = f'sayles_{prune_type}_trial{trial}_seg{seg}_{ntwk_window_size_ms}ms.p'\n",
    "        leadership_seg = pickle.load( open(f'../data/sayles_leadership_Oscar/{filename_seg}', 'rb') )\n",
    "\n",
    "        for measure_type in measure_types:\n",
    "            leadership[measure_type][seg] = leadership_seg[measure_type][seg]\n",
    "            leadership[measure_type+'rank'][seg] = leadership_seg[measure_type+'rank'][seg]\n",
    "\n",
    "    filename_trial = f'sayles_{prune_type}_trial{trial}_{ntwk_window_size_ms}ms.p'\n",
    "    # e.g., leadership['NIrank'][seg][ntwk,i]\n",
    "    pickle.dump(leadership, open(f'../data/sayles_leadership_Oscar/{filename_trial}', 'wb'))\n",
    "    print(filename_trial, 'saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# ===== CHANGE THIS ========================================\n",
    "ntwk_window_sizes = [0.5]   # , 1\n",
    "prune_type = 'pruned'       # unpruned or pruned\n",
    "# ==========================================================\n",
    "\n",
    "measure_types = ['NI', 'NBI', 'CI', 'CBI']\n",
    "\n",
    "# the size of network time window (in s)\n",
    "for ntwk_window_size in ntwk_window_sizes:\n",
    "    print('compiling leadership for', prune_type, 'networks (network window size:', ntwk_window_size, 's)')\n",
    "    ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "\n",
    "    leadership = {}\n",
    "    for measure_type in measure_types:\n",
    "        leadership[measure_type] = {}\n",
    "        leadership[measure_type+'rank'] = {}\n",
    "\n",
    "    for trial in range(1,13):\n",
    "        filename = f'sayles_{prune_type}_trial{trial}_{ntwk_window_size_ms}ms.p'\n",
    "        leadership_trial = pickle.load( open(f'../data/sayles_leadership_Oscar/{filename}', 'rb') )\n",
    "\n",
    "        for measure_type in leadership_trial.keys():\n",
    "            leadership[measure_type][trial] = leadership_trial[measure_type]\n",
    "\n",
    "    # e.g., leadership['NIrank'][trial][seg][ntwk,i]\n",
    "    pickle.dump(leadership, open(f'../data/pickle/sayles_leadership_{prune_type}_{ntwk_window_size_ms}ms.p', 'wb'))\n",
    "\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run this section to produce leadership measures in unpruned networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init\n",
    "import numpy as np\n",
    "import pickle\n",
    "import config\n",
    "from utils.get_leadership import get_NI, get_NBI, get_CI, get_CBI, get_rank\n",
    "\n",
    "## ====== change ONLY HERE ============\n",
    "ntwk_window_size = config.NTWK_WINDOW_SIZE   # in s\n",
    "measure_types = ['NI', 'NBI']\n",
    "## ====================================\n",
    "\n",
    "ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "weights = pickle.load( open(f'../data/pickle/sayles_weights_unpruned_{ntwk_window_size_ms}ms.p', 'rb') )\n",
    "\n",
    "print('Computing leadership measure types', measure_types)\n",
    "print('for unpruned networks', '(network window size:', ntwk_window_size, 's)\\n')\n",
    "\n",
    "leadership = {}\n",
    "\n",
    "for measure_type in measure_types:\n",
    "    print(measure_type, end=':')\n",
    "    leadership[measure_type] = {}\n",
    "    leadership[measure_type+'rank'] = {}\n",
    "\n",
    "    if measure_type=='NI':\n",
    "        get_leadership = get_NI\n",
    "    elif measure_type=='NBI':\n",
    "        get_leadership = get_NBI\n",
    "    elif measure_type=='CI':\n",
    "        get_leadership = get_CI\n",
    "    elif measure_type=='CBI':\n",
    "        get_leadership = get_CBI\n",
    "    else:\n",
    "        raise ValueError('measure type error')\n",
    "\n",
    "    for trial in weights.keys():\n",
    "        print('trial', int(trial))\n",
    "        leadership[measure_type][trial] = {}\n",
    "        leadership[measure_type+'rank'][trial] = {}\n",
    "\n",
    "        for seg in weights[trial].keys():\n",
    "            print('-- segment', seg)\n",
    "            num_networks, N, _ = weights[trial][seg].shape    # (num_networks, N, N)\n",
    "\n",
    "            leadership[measure_type][trial][seg] = np.zeros((num_networks, N))\n",
    "            leadership[measure_type+'rank'][trial][seg] = np.zeros((num_networks, N))\n",
    "\n",
    "            for ntwk in range(num_networks):\n",
    "                print(ntwk+1, end=' ')\n",
    "                leadership[measure_type][trial][seg][ntwk,:] = get_leadership(weights[trial][seg][ntwk,:,:], order=\"ij\")  # (N,)\n",
    "                leadership[measure_type+'rank'][trial][seg][ntwk,:] = get_rank(leadership[measure_type][trial][seg][ntwk,:])  # (N,)\n",
    "\n",
    "            print(end='\\n')\n",
    "    print(end='\\n')\n",
    "\n",
    "# e.g., leadership['NIrank'][trial][seg][ntwk,i]\n",
    "pickle.dump(leadership, open(f'../data/pickle/sayles_leadership_unpruned_{ntwk_window_size_ms}ms.p', 'wb'))\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot leadership dynamics\n",
    "Plot how leadership ranking change over time in each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dynamics (with lines & dots)\n",
    "\n",
    "##  init \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from utils.plot_leadership import plot_leadership_dynamics\n",
    "import config\n",
    "\n",
    "# ===== CHANGE THIS ========================================\n",
    "ntwk_window_size = config.NTWK_WINDOW_SIZE   # in s\n",
    "prune_type = 'pruned'       # unpruned or pruned\n",
    "# ==========================================================\n",
    "\n",
    "ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "leadership = pickle.load( open('../data/pickle/sayles_leadership_'+prune_type+'_'\n",
    "                             +str(ntwk_window_size_ms)+'ms.p', 'rb') )\n",
    "df = pd.read_csv('../data/csv/sayles_data_90pct.csv')\n",
    "\n",
    "print(\"plotting leadership dynamics (\" + str(ntwk_window_size) + \"s)\")\n",
    "\n",
    "for measure_type in leadership.keys():\n",
    "    if measure_type[-4:]!='rank':\n",
    "        continue\n",
    "    print(measure_type)\n",
    "    \n",
    "    for trial in leadership[measure_type].keys():\n",
    "        print('trial', int(trial), '-- segment', end=' ')\n",
    "        trial_df = df[(df['trial']==trial)]\n",
    "\n",
    "        for seg in leadership[measure_type][trial].keys():\n",
    "            print(seg, end=' ')\n",
    "            for form in ['png', 'svg']:\n",
    "                plot_leadership_dynamics(leadership[measure_type][trial][seg], measure_type[:2],\n",
    "                                         description=f'(Trial {trial} Segment {seg})',\n",
    "                                         saved=True,\n",
    "                                         output_folder='../output/sayles/'+form+'/leadership_dynamics_dots/'+measure_type+'_'+prune_type+'_'+str(ntwk_window_size_ms)+'ms/',\n",
    "                                         output_file='rankings_trial'+str(trial)+'_seg'+str(seg),\n",
    "                                         output_format=form)\n",
    "        print(end='\\n') \n",
    "    \n",
    "print('saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a specific segment & speficied networks\n",
    "\n",
    "# plot dynamics (with lines & dots)\n",
    "\n",
    "##  init \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from utils.plot_leadership import plot_leadership_dynamics\n",
    "import config\n",
    "\n",
    "# ===== CHANGE THIS ========================================\n",
    "ntwk_window_size = config.NTWK_WINDOW_SIZE   # in s\n",
    "prune_type = 'pruned'       # unpruned or pruned\n",
    "\n",
    "# specify networks to plot\n",
    "trial = 10 #5\n",
    "seg = 1 #5\n",
    "start_ntwk_ID = 13 #11  # plot network IDs between start_network and end_network\n",
    "end_ntwk_ID = 32 #15\n",
    "# ==========================================================\n",
    "\n",
    "ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "leadership = pickle.load( open('../data/pickle/sayles_leadership_'+prune_type+'_'\n",
    "                             +str(ntwk_window_size_ms)+'ms.p', 'rb') )\n",
    "df = pd.read_csv('../data/csv/sayles_data_90pct.csv')\n",
    "\n",
    "print(\"plotting leadership dynamics (\" + str(ntwk_window_size) + \"s)\")\n",
    "print(f'trial {trial} segment {seg}, network {start_ntwk_ID} - {end_ntwk_ID}')\n",
    "\n",
    "for measure_type in leadership.keys():\n",
    "    if measure_type[-4:]!='rank':\n",
    "        continue\n",
    "    print(measure_type)\n",
    "    \n",
    "    trial_df = df[(df['trial']==trial)]\n",
    "\n",
    "    # leadership[measure_type][trial][seg] : (num_networks, N)\n",
    "    for form in ['png', 'svg']:\n",
    "        plot_leadership_dynamics(leadership[measure_type][trial][seg][start_ntwk_ID:end_ntwk_ID+1], measure_type[:2],\n",
    "                                 description=f'(Trial {trial} Segment {seg})',\n",
    "                                 saved=True,\n",
    "                                 output_folder=f'../output/sayles/{form}/leadership_dynamics_dots/{measure_type}_{prune_type}_{ntwk_window_size_ms}ms/publication/',\n",
    "                                 output_file=f'rankings_trial{trial}_seg{seg}_ntwk{start_ntwk_ID}to{end_ntwk_ID}',\n",
    "                                 output_format=form)\n",
    "    \n",
    "print('saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot rank correlation\n",
    "\n",
    "Based on *code/plot_rank_correlation.m* from repo *FYP*.\n",
    "Evaluate rank correlation (Spearman's rho)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rank-order correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from utils.get_network import get_network_weights\n",
    "from utils.get_leadership import get_NI, get_rank\n",
    "import config\n",
    "\n",
    "## ----- change only here -----------------------------------------------------------------\n",
    "# the max separation between the correlated networks (in terms of the number of networks apart)\n",
    "max_apart = 10\n",
    "# leadership measure to use for calculating leadership ranks\n",
    "measure_type = 'NI'\n",
    "## --------------------------------------------------------------------------------\n",
    "\n",
    "# sample frequency\n",
    "SAMP_FREQ = config.SAMP_FREQ\n",
    "# tau\n",
    "tau = config.TAU\n",
    "# sizes of network time window to consider\n",
    "ntwk_window_sizes = [0.5, 1, 2, 3, 4]\n",
    "\n",
    "## ===== init =====\n",
    "df = pd.read_csv('../data/csv/sayles_data_90pct.csv')\n",
    "index = pickle.load( open( \"../data/pickle/index.p\", \"rb\" ) )\n",
    "## initialize vars\n",
    "# store lists of rho to average over (same shape as avg_rho but a list)\n",
    "ls_rho = [ [ [] for _ in range(max_apart) ] for _ in range(len(ntwk_window_sizes)) ]\n",
    "\n",
    "## ===== calculate ==========\n",
    "for trial in index.keys():\n",
    "    print('trial', int(trial), '-- segment', end=' ')\n",
    "    trial_df = df[(df['trial']==trial)]\n",
    "\n",
    "    for seg in index[trial].keys():\n",
    "        print(seg, end=' ')\n",
    "        seg_df = trial_df[trial_df['segment']==seg]\n",
    "\n",
    "        x = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                         values='x').to_numpy()\n",
    "        y = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                         values='y').to_numpy()\n",
    "        heading = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                               values='heading').to_numpy()\n",
    "\n",
    "        for idx, ws in enumerate(ntwk_window_sizes):    # index, window size\n",
    "            ## ----- get network weights for the segment -----\n",
    "            # weights: (num_networks, N, N)\n",
    "            pruned_weights_seg = get_network_weights(index[trial][seg],\n",
    "                                                     x, y, heading, SAMP_FREQ,\n",
    "                                                     pruning=True,\n",
    "                                                     TAU=tau,\n",
    "                                                     NTWK_WINDOW_SIZE=ws)\n",
    "            \n",
    "            ## ----- get NI ranks for the segment -----\n",
    "            num_networks, N, _ = pruned_weights_seg.shape    # (num_networks, N, N)\n",
    "            if (ws >= num_networks):\n",
    "                break\n",
    "            # unpruned_ranks_seg = np.zeros( (num_networks, N) )\n",
    "            pruned_ranks_seg = np.zeros( (num_networks, N) )\n",
    "            # print('(network', end=' ')\n",
    "            for ntwk in range(num_networks):\n",
    "                if measure_type == 'NI':\n",
    "                    get_leadership = get_NI\n",
    "                else:\n",
    "                    raise ValueError('measure type error :', measure_type) \n",
    "                # NI values & ranks in the given network\n",
    "                pruned_L_seg = get_leadership(pruned_weights_seg[ntwk,:,:], order=\"ij\")  # NI values: (N,)\n",
    "                pruned_ranks_seg[ntwk,:] = get_rank(pruned_L_seg)  # (N,)\n",
    "\n",
    "            ## ----- get rank-order correlation (rho) for all segments -----\n",
    "            # from calculateRankCorrMat()\n",
    "            rho, pval = spearmanr(pruned_ranks_seg, axis=1, nan_policy='omit')    # (num_networks, num_networks)\n",
    "\n",
    "            ## ----- get average rho -----\n",
    "            # from getAvgRankCorr()\n",
    "            for ntwks_apart in range(1, min(max_apart+1, num_networks)):     # number of networks apart\n",
    "                # print(ntwks_apart, 'networks apart')\n",
    "                mask = np.eye(num_networks, k=ntwks_apart, dtype=bool)\n",
    "                if isinstance(rho, np.ndarray):  # add only if rho is a matrix\n",
    "                    ls_rho[idx][ntwks_apart-1].extend(rho[mask])\n",
    "    print(end='\\n')\n",
    "\n",
    "## ----- get average rho -----\n",
    "# how many correlations each average rho represents\n",
    "num_datapoints = np.zeros( (len(ntwk_window_sizes), max_apart) )\n",
    "# average rho to plot\n",
    "avg_rho = np.zeros( (len(ntwk_window_sizes), max_apart) )\n",
    "for ws in range(len(ntwk_window_sizes)):\n",
    "    for ma in range(max_apart):\n",
    "        num_datapoints[ws][ma] = len(ls_rho[ws][ma])\n",
    "        avg_rho[ws,ma] = np.nanmean(ls_rho[ws][ma])\n",
    "\n",
    "var = {}\n",
    "var['avg_rho'] = avg_rho    # (len(ntwk_window_sizes), max_apart)\n",
    "var['num_datapoints'] = num_datapoints  # (len(ntwk_window_sizes), max_apart)\n",
    "\n",
    "pickle.dump( var, open( \"../data/pickle/sayles_avg_rho.p\", \"wb\" ) )\n",
    "print(\"saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print overall rank-order correlation (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|   Window size |   1 apart |   2 apart |   3 apart |   4 apart |   5 apart |   6 apart |   7 apart |   8 apart |   9 apart |   10 apart |\n",
      "+===============+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+\n",
      "|           0.5 |  0.892584 |  0.819012 |  0.763372 |  0.717933 |  0.680128 |  0.646641 |  0.617061 |  0.590499 |  0.567404 |   0.54491  |\n",
      "+---------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|           1   |  0.843687 |  0.739546 |  0.666895 |  0.608314 |  0.560875 |  0.513802 |  0.466821 |  0.422876 |  0.375296 |   0.337593 |\n",
      "+---------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|           2   |  0.784592 |  0.642269 |  0.540108 |  0.433331 |  0.343382 |  0.304105 |  0.262512 |  0.2266   |  0.225223 |   0.222327 |\n",
      "+---------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|           3   |  0.739794 |  0.559709 |  0.419504 |  0.335131 |  0.265454 |  0.255952 |  0.262268 |  0.225886 |  0.233316 |   0.225204 |\n",
      "+---------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n",
      "|           4   |  0.701891 |  0.488611 |  0.359441 |  0.270402 |  0.261218 |  0.238848 |  0.237919 |  0.213806 |  0.197911 |   0.218316 |\n",
      "+---------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "var = pickle.load( open( \"../data/pickle/sayles_avg_rho.p\", \"rb\" ) )\n",
    "avg_rho = var['avg_rho']     # (len(ntwk_window_sizes), max_apart)\n",
    "num_datapoints = var['num_datapoints']  # (len(ntwk_window_sizes), max_apart)\n",
    "\n",
    "num_window_sizes, max_apart = avg_rho.shape\n",
    "x = np.arange(1, max_apart+1)\n",
    "\n",
    "# sizes of network time window to consider\n",
    "ntwk_window_sizes = [0.5, 1, 2, 3, 4]\n",
    "\n",
    "data = []\n",
    "for i in range(num_window_sizes):\n",
    "    data.append([ntwk_window_sizes[i]] + list(avg_rho[i,:]))\n",
    "\n",
    "head = [\"Window size\"]\n",
    "for i in range(1, max_apart+1):\n",
    "    head.append(str(i) + \" apart\")\n",
    " \n",
    "print(tabulate(data, headers=head, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot overall rank-order correlation (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "var = pickle.load( open( \"../data/pickle/sayles_avg_rho.p\", \"rb\" ) )\n",
    "avg_rho = var['avg_rho']     # (len(ntwk_window_sizes), max_apart)\n",
    "num_datapoints = var['num_datapoints']  # (len(ntwk_window_sizes), max_apart)\n",
    "\n",
    "num_window_sizes, max_apart = avg_rho.shape\n",
    "x = np.arange(1, max_apart+1)\n",
    "\n",
    "# sizes of network time window to consider\n",
    "ntwk_window_sizes = [0.5, 1, 2, 3, 4]\n",
    "\n",
    "## ===== plot avg_rho as a function on time =====\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "for i in range(num_window_sizes):\n",
    "    ax.plot(x * ntwk_window_sizes[i], avg_rho[i,:], \n",
    "            label=f'{ntwk_window_sizes[i]} seconds', marker='o')\n",
    "\n",
    "ax.set_xlabel('Seconds apart', fontsize=15)\n",
    "ax.set_ylabel(r'Average rank correlation ($\\rho$)', fontsize=15)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# ax.set_title('Average rank correlation vs. time apart : overall', fontsize=18)\n",
    "ax.set_xlim(0, 10)  # max\n",
    "ax.set_ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()  # Adjust layout to fit legend\n",
    "\n",
    "for form in ['png', 'svg']:\n",
    "    new_folder = f'../output/sayles/{form}/rank_correlation/'\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "    plt.savefig(os.path.join(new_folder, 'rank_correlation_time.' + form))\n",
    "# plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot spatial heat maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get values needed for plotting spatial heat maps\n",
    "Collect leadership values & corresponding positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import config\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from utils.plot_spatial_heatmap import *\n",
    "\n",
    "# ===== CHANGE THIS ===================================\n",
    "# specify grid width (m or SD) in spatial heatmaps\n",
    "grid_width = {}\n",
    "grid_width['m'] = 0.5\n",
    "grid_width['SD'] = 0.25\n",
    "# network window size\n",
    "ntwk_window_size = config.NTWK_WINDOW_SIZE   # in s\n",
    "# prune_type = 'pruned'       # unpruned or pruned\n",
    "\n",
    "# prob_threshold = 0.005\n",
    "prob_threshold = {}\n",
    "prob_threshold['N16'] = 0.005  # has more outliers\n",
    "prob_threshold['N20'] = 0.002;  prob_threshold['N10'] = 0.002\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT =====\n",
    "# --- get data ---\n",
    "df = pd.read_csv('../data/csv/sayles_data_90pct.csv')\n",
    "groups = ['N16', 'N20', 'N10']\n",
    "conditions = ['overall'] + groups\n",
    "units = ['m', 'SD']\n",
    "axes = ['x', 'y']\n",
    "prune_types = ['unpruned', 'pruned']\n",
    "\n",
    "ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "SAMP_FREQ = config.SAMP_FREQ\n",
    "leadership = {}\n",
    "for prune_type in prune_types:\n",
    "    # leadership[prune_type][measure_type][trial][seg] : (num_networks, N)\n",
    "    leadership[prune_type] = pickle.load( open(f'../data/pickle/sayles_leadership_{prune_type}_{ntwk_window_size_ms}ms.p', 'rb') )\n",
    "num_frames_network = int(SAMP_FREQ * ntwk_window_size)\n",
    "\n",
    "# --- initialize variables ---\n",
    "# positions[cond][ax][unit] : a python list for positions \n",
    "# shape : (num_datapoints,)\n",
    "positions = {}\n",
    "for cond in conditions:\n",
    "    positions[cond] = {}\n",
    "    for ax in axes: \n",
    "        positions[cond][ax] = {}\n",
    "        for unit in units:\n",
    "            positions[cond][ax][unit] = []     # values will be added to this list\n",
    "# values[cond][prune_type][measure_type] : a python list of leadership value\n",
    "# shape : (num_datapoints,)\n",
    "values = {}\n",
    "for cond in conditions:\n",
    "    values[cond] = {}\n",
    "    for prune_type in prune_types:\n",
    "        values[cond][prune_type] = {}\n",
    "        for measure_type in leadership[prune_type].keys():\n",
    "            values[cond][prune_type][measure_type] = []     # values will be added to this list\n",
    "temp_all_x_m, temp_all_y_m = [], []\n",
    "# PDF[cond][unit] : a numpy array of leadership value\n",
    "# shape : (num_y_cells, num_x_cells) -> this gets rotated when plotting\n",
    "PDF = {}\n",
    "for cond in conditions:\n",
    "    PDF[cond] = {}\n",
    "    for unit in units:\n",
    "        PDF[cond][unit] = []    # this list will be replaced with np array\n",
    "# min/max positions on X & Y axes\n",
    "lims = ['xmin', 'xmax', 'ymin', 'ymax']\n",
    "position_limits = {}\n",
    "for unit in units:\n",
    "    position_limits[unit] = {}\n",
    "    for lim in lims:\n",
    "        position_limits[unit][lim] = float('inf') if (lim[1:]=='min') else float('-inf')\n",
    "\n",
    "# ===== collect positions (in meters) & leadership values per group =====\n",
    "temp_prune = prune_types[0]\n",
    "temp_measure = list(leadership[prune_types[0]].keys())[0]   # this is only used to access general variable shapes\n",
    "\n",
    "for trial in leadership[temp_prune][temp_measure].keys():\n",
    "    for seg in leadership[temp_prune][temp_measure][trial].keys():\n",
    "        num_networks, N = leadership[temp_prune][temp_measure][trial][seg].shape\n",
    "        group = 'N' + str(N)\n",
    "\n",
    "        ## ----- get positions (meters) -----\n",
    "        seg_df = df[(df['trial']==trial) & (df['segment']==seg)]\n",
    "        # get node positions (absolute; in m) in the given segment\n",
    "        x_M_seg = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                               values='x_transformed').to_numpy()\n",
    "        y_M_seg = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                        values='y_transformed').to_numpy()\n",
    "        # get node positions (absolute; in m) in the 1st frame of each time window per network\n",
    "        x_M = x_M_seg[::num_frames_network, :][:num_networks,:].flatten()   # (num_networks, N) -> flattened\n",
    "        y_M = y_M_seg[::num_frames_network, :][:num_networks,:].flatten()   # (num_networks, N) -> flattened\n",
    "        # get a mask\n",
    "        temp_value = leadership[temp_prune][temp_measure][trial][seg].flatten()      # (num_networks, N) -> flattened\n",
    "        mask = ~np.isnan(temp_value) * ~np.isnan(x_M) * ~np.isnan(y_M)   # look for nan\n",
    "        # add positions to lists\n",
    "        positions[group]['x']['m'] += list(x_M[mask])\n",
    "        positions[group]['y']['m'] += list(y_M[mask])\n",
    "        # add to temporary lists; used to calculate position_limits\n",
    "        temp_all_x_m += list(x_M[mask])   \n",
    "        temp_all_y_m += list(y_M[mask])\n",
    "\n",
    "        ## ----- get leadership values  -----\n",
    "        for prune_type in prune_types:\n",
    "            for measure_type in leadership[prune_type].keys():  # get leadership values for each measure type\n",
    "                v = leadership[prune_type][measure_type][trial][seg].flatten()    # (num_networks, N) -> flattened\n",
    "                if (measure_type[-4:] == \"rank\"):  # flip ranking (higher value = higher rank)\n",
    "                    v = (N + 1 - v) / N\n",
    "                # add leadership values to lists\n",
    "                values[group][prune_type][measure_type] += list(v[mask])\n",
    "\n",
    "# ===== calculate PDF per group & remove outlier datapoints =====\n",
    "total_datapoints = 0\n",
    "filtered_datapoints = 0\n",
    "print('filtering out outlier datapoints...')\n",
    "\n",
    "def count_data_points(x, y, \n",
    "                      xmin, xmax, ymin, ymax,\n",
    "                      grid_width=0.5,\n",
    "                      threshold=5):\n",
    "    x_edges = np.arange(xmin, xmax + grid_width, grid_width)\n",
    "    y_edges = np.arange(ymin, ymax + grid_width, grid_width)\n",
    "    counts, _, _ = np.histogram2d(x, y, bins=[x_edges, y_edges])\n",
    "    return counts < threshold\n",
    "\n",
    "for group in groups:\n",
    "    # --- find outliers based on cell probability threshold ---\n",
    "    mask = count_data_points(positions[group]['x']['m'], \n",
    "                             positions[group]['y']['m'], \n",
    "                             int(np.floor(np.min(positions[group]['x']['m']))), \n",
    "                             int(np.ceil(np.max(positions[group]['x']['m']))),\n",
    "                             int(np.floor(np.min(positions[group]['y']['m']))), \n",
    "                             int(np.ceil(np.max(positions[group]['y']['m']))),\n",
    "                             grid_width=grid_width['m'])\n",
    "    print(mask)\n",
    "    # --- count filtered datapoints ---\n",
    "    print(f'{group} (probability threshold = {prob_threshold[group]}) : ', end='')\n",
    "    print(f'{round((~mask).sum()/len(mask)*100, 2)}% of datapoints removed (total: {len(mask)}, filtered: {(~mask).sum()})')\n",
    "    # print(f\"{group}  :\", end=\" \")\n",
    "    total_datapoints += len(mask)\n",
    "    filtered_datapoints += (~mask).sum()\n",
    "    # --- update xy poitions (in m) & leadership (filter outliers out) ---\n",
    "    positions[group]['x']['m'] = list(np.array(positions[group]['x']['m'])[mask])\n",
    "    positions[group]['y']['m'] = list(np.array(positions[group]['y']['m'])[mask])\n",
    "    for prune_type in prune_types:\n",
    "        for measure_type in leadership[prune_type].keys():\n",
    "            values[group][prune_type][measure_type] = list(np.array(values[group][prune_type][measure_type])[mask])\n",
    "    positions['overall']['x']['m'] += positions[group]['x']['m']\n",
    "    positions['overall']['y']['m'] += positions[group]['y']['m']\n",
    "\n",
    "print(f'all groups: {round(filtered_datapoints/total_datapoints*100, 2)}% of datapoints removed (total: {total_datapoints}, filtered: {filtered_datapoints})\\n')\n",
    "position_limits['m']['xmin'] = int(np.floor(np.min(positions['overall']['x']['m'])))\n",
    "position_limits['m']['xmax'] = int(np.ceil(np.max(positions['overall']['x']['m'])))\n",
    "position_limits['m']['ymin'] = int(np.floor(np.min(positions['overall']['y']['m'])))\n",
    "position_limits['m']['ymax'] = int(np.ceil(np.max(positions['overall']['y']['m'])))\n",
    "\n",
    "# ===== calculate PDF & get SDs per group =====\n",
    "print('SDs on x & y axes per group...')\n",
    "for group in groups:\n",
    "    # --- recalculate PDF to get SD per group ---\n",
    "    SD_width_x, SD_width_y, PDF[group]['m'] = get_SD(positions[group]['x']['m'], positions[group]['y']['m'],\n",
    "                                                     xmin=position_limits['m']['xmin'], \n",
    "                                                     xmax=position_limits['m']['xmax'], \n",
    "                                                     ymin=position_limits['m']['ymin'], \n",
    "                                                     ymax=position_limits['m']['ymax'], \n",
    "                                                     grid_width=grid_width['m'])\n",
    "    print(group, \":\", end=\" \")\n",
    "    print(SD_width_x, SD_width_y)\n",
    "    # --- transform the coordinates (m -> SD) ---\n",
    "    temp_x_SD, temp_y_SD = transform_m2SD(positions[group]['x']['m'], positions[group]['y']['m'], \n",
    "                                          SD_width_x, SD_width_y)\n",
    "    positions[group]['x']['SD'] = list(temp_x_SD)\n",
    "    positions[group]['y']['SD'] = list(temp_y_SD)\n",
    "    \n",
    "# ===== combine all groups for overall heat mapss =====\n",
    "for group in groups:\n",
    "    # add position data\n",
    "    for ax in axes: \n",
    "        # for unit in units:\n",
    "        positions['overall'][ax]['SD'] += positions[group][ax]['SD']\n",
    "    # add leadership values\n",
    "    for prune_type in prune_types:\n",
    "        for measure_type in leadership[prune_type].keys():\n",
    "            values['overall'][prune_type][measure_type] += values[group][prune_type][measure_type]\n",
    "\n",
    "\n",
    "# ===== collect overall position limits (in m & SD) =====\n",
    "print(\"\\nposition limits (rounded)...\")\n",
    "position_limits['SD']['xmin'] = int(np.floor(np.min(positions['overall']['x']['SD'])))\n",
    "position_limits['SD']['xmax'] = int(np.ceil(np.max(positions['overall']['x']['SD'])))\n",
    "position_limits['SD']['ymin'] = int(np.floor(np.min(positions['overall']['y']['SD'])))\n",
    "position_limits['SD']['ymax'] = int(np.ceil(np.max(positions['overall']['y']['SD'])))\n",
    "\n",
    "# ===== get probability density =====\n",
    "for unit in units:\n",
    "    PDF['overall'][unit] = get_PDF(positions['overall']['x'][unit], positions['overall']['y'][unit],\n",
    "                                   xmin=position_limits[unit]['xmin'], xmax=position_limits[unit]['xmax'], \n",
    "                                   ymin=position_limits[unit]['ymin'], ymax=position_limits[unit]['ymax'], \n",
    "                                   grid_width=grid_width[unit])\n",
    "for group in groups:\n",
    "    PDF[group]['SD'] = get_PDF(positions[group]['x']['SD'], positions[group]['y']['SD'], \n",
    "                               xmin=position_limits['SD']['xmin'], xmax=position_limits['SD']['xmax'], \n",
    "                               ymin=position_limits['SD']['ymin'], ymax=position_limits['SD']['ymax'], \n",
    "                               grid_width=grid_width['SD'])\n",
    "\n",
    "for unit in units:\n",
    "    print(unit, list(position_limits[unit].keys()), \":\", end=\" \")\n",
    "    print(position_limits[unit]['xmin'],\n",
    "          position_limits[unit]['xmax'],\n",
    "          position_limits[unit]['ymin'],\n",
    "          position_limits[unit]['ymax'] )\n",
    "\n",
    "# ===== check shapes =====\n",
    "# for cond in conditions:\n",
    "print('checking matrix shapes...')\n",
    "for cond in ['overall']:\n",
    "    print(cond)\n",
    "    for unit in units:\n",
    "        print(\"---\", unit, \":\")\n",
    "        print(\"   \", PDF[cond][unit].shape)\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate average leadership values for each cell in spatial heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===== init =====\n",
    "# specify bins on both axes (e.g., [-3, 4]: 0.5 step)\n",
    "x_range, y_range = {}, {}\n",
    "for unit in units:\n",
    "    x_range[unit] = list(np.arange(position_limits[unit]['xmin'], \n",
    "                                   position_limits[unit]['xmax'] + grid_width[unit], \n",
    "                                   grid_width[unit]))     \n",
    "    y_range[unit] = list(np.arange(position_limits[unit]['ymax'], \n",
    "                                   position_limits[unit]['ymin'] - grid_width[unit], \n",
    "                                   -grid_width[unit]))\n",
    "\n",
    "output_leadership_avg = {}\n",
    "\n",
    "# some cells with no data end up being a empty list, which gives \"RuntimeWarning: Mean of empty slice\"\n",
    "# but this can be ignored\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "## ===== calculate average leadership in each cell =====\n",
    "for cond in conditions:\n",
    "    # print(cond)\n",
    "    output_leadership_avg[cond] = {}\n",
    "\n",
    "    for unit in units:\n",
    "        # print(\"---\", unit)\n",
    "        output_leadership_avg[cond][unit] = {}\n",
    "        # get cell indices for each datapoint (num_x_values = num_y_values)\n",
    "        x_digitized = np.digitize(positions[cond]['x'][unit], np.array(x_range[unit]))\n",
    "        y_digitized = np.digitize(positions[cond]['y'][unit], np.array(y_range[unit]))\n",
    "\n",
    "        # get leadership value for each data and add to the list of corresponding cell\n",
    "        for prune_type in prune_types:\n",
    "            # print(\"------\", prune_type, \":\", end=\" \")\n",
    "            output_leadership_avg[cond][unit][prune_type] = {}\n",
    "            \n",
    "            for measure_type in leadership[prune_type].keys():\n",
    "                # print(measure_type, end=\" \")\n",
    "                # output_leadership_avg[cond][unit][measure_type] = [[[] for _ in range(len(y_range))] for _ in range(len(x_range))]\n",
    "                temp_ls = [[[] for _ in range(len(y_range[unit]))] for _ in range(len(x_range[unit]))]\n",
    "                # collect values for each cell\n",
    "                for d in range(x_digitized.shape[0]):  # for each datapoint\n",
    "                    temp_ls[x_digitized[d]][y_digitized[d]].append(values[cond][prune_type][measure_type][d])\n",
    "                # calculate the leadership average in each cell\n",
    "                output_leadership_avg[cond][unit][prune_type][measure_type] = np.zeros((len(x_range[unit]), len(y_range[unit])))\n",
    "                for x_ind in range(len(x_range[unit])):\n",
    "                    for y_ind in range(len(y_range[unit])):\n",
    "                        output_leadership_avg[cond][unit][prune_type][measure_type][x_ind, y_ind] = np.nanmean(temp_ls[x_ind][y_ind])\n",
    "            # print()\n",
    "\n",
    "# ===== check shapes =====\n",
    "isError = False\n",
    "for cond in conditions:\n",
    "    for unit in units:\n",
    "        for prune_type in prune_types:\n",
    "            for measure_type in leadership[prune_type].keys():\n",
    "                if (PDF[cond][unit].shape != output_leadership_avg[cond][unit][prune_type][measure_type].shape):\n",
    "                    isError = True\n",
    "                    print(\"error:\", cond, unit, prune_type, measure_type)\n",
    "                    print(\"PDF shape:\", PDF[cond][unit].shape)\n",
    "                    print(\"output_leadership_avg shape:\", output_leadership_avg[cond][unit][prune_type][measure_type].shape)\n",
    "                    \n",
    "if not isError:\n",
    "    print(\"no error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot spatial heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('plotting...')\n",
    "\n",
    "# ===== change this =====\n",
    "saved = True\n",
    "# =======================\n",
    "\n",
    "for cond in conditions:\n",
    "    print(cond, end=' : ')\n",
    "    for unit in units:\n",
    "        print(unit, end=' ')\n",
    "        # plot PDF\n",
    "        for form in ['png', 'svg']:\n",
    "            plot_PDF(PDF[cond][unit],\n",
    "                     position_limits[unit][\"xmin\"], \n",
    "                     position_limits[unit][\"xmax\"], \n",
    "                     position_limits[unit][\"ymin\"], \n",
    "                     position_limits[unit][\"ymax\"],\n",
    "                     unit=unit,\n",
    "                     cmap_name=\"jet\",\n",
    "                     grid_width=grid_width[unit],\n",
    "                     saved=saved,\n",
    "                     plotGaussian=True, \n",
    "                     output_folder=f'../output/sayles/{form}/spatial_heatmaps/PDF/{unit}_{ntwk_window_size_ms}ms/',\n",
    "                     output_file=f'sayles_{unit}_{cond}',\n",
    "                     output_format=form)\n",
    "        for prune_type in prune_types:\n",
    "            for measure_type in leadership[prune_type].keys():\n",
    "                for form in ['png', 'svg']:\n",
    "                    # plot leadership\n",
    "                    plot_leadership_heatmap(output_leadership_avg[cond][unit][prune_type][measure_type], \n",
    "                                            position_limits[unit][\"xmin\"], position_limits[unit][\"xmax\"], \n",
    "                                            position_limits[unit][\"ymin\"], position_limits[unit][\"ymax\"],\n",
    "                                            unit=unit,\n",
    "                                            grid_width=grid_width[unit],\n",
    "                                            saved=saved,\n",
    "                                            title=measure_type,\n",
    "                                            output_folder=f'../output/sayles/{form}/spatial_heatmaps/leadership/{unit}_{prune_type}_{ntwk_window_size_ms}ms/',\n",
    "                                            output_file=f'sayles_{measure_type}_{unit}_{cond}',\n",
    "                                            output_format=form)\n",
    "    print()\n",
    "                \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot individual leadership index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the relationship between postion & individual leadership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import config\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from utils.plot_spatial_heatmap import *\n",
    "from utils.plot_leadership import plot_confidence_ellipse\n",
    "\n",
    "# ===== CHANGE THIS ===================================\n",
    "# specify grid width (m or SD) in spatial heatmaps\n",
    "# kde_grid_width = 0.5\n",
    "grid_width = {}\n",
    "grid_width['m'] = 0.5\n",
    "grid_width['SD'] = 0.25\n",
    "# network window size\n",
    "ntwk_window_size = config.NTWK_WINDOW_SIZE   # in s\n",
    "prune_type = 'pruned'       # unpruned or pruned\n",
    "n_std = 1   # radius (SD) of confidence ellipse\n",
    "\n",
    "# prob_threshold = 0.005\n",
    "prob_threshold = {}\n",
    "prob_threshold['N16'] = 0.002;  prob_threshold['N20'] = 0.005;  prob_threshold['N10'] = 0.005\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot SD-based position values\n",
    "\n",
    "# ===== INIT =====\n",
    "# --- get data ---\n",
    "df = pd.read_csv('../data/csv/sayles_data_90pct.csv')\n",
    "groups = ['N16', 'N20', 'N10']\n",
    "conditions = ['overall'] + groups\n",
    "units = ['m', 'SD']\n",
    "axes = ['x', 'y']\n",
    "\n",
    "ntwk_window_size_ms = int(ntwk_window_size * 1000)\n",
    "SAMP_FREQ = config.SAMP_FREQ\n",
    "leadership = {}\n",
    "leadership = pickle.load( open('../data/pickle/sayles_leadership_' + prune_type + '_' + str(ntwk_window_size_ms) + 'ms.p', 'rb') )\n",
    "num_frames_network = int(SAMP_FREQ * ntwk_window_size)\n",
    "measure_types = leadership.keys()\n",
    "\n",
    "# --- initialize variables ---\n",
    "# positions[cond][ax][unit] : a python list for positions \n",
    "# shape : (num_datapoints,)\n",
    "positions = {}\n",
    "for cond in conditions:\n",
    "    positions[cond] = {}\n",
    "    for ax in axes: \n",
    "        positions[cond][ax] = {}\n",
    "        for unit in units:\n",
    "            positions[cond][ax][unit] = []     # values will be added to this list\n",
    "temp_all_x_m, temp_all_y_m = [], []\n",
    "# min/max positions on X & Y axes\n",
    "lims = ['xmin', 'xmax', 'ymin', 'ymax']\n",
    "position_limits = {}\n",
    "for unit in units:\n",
    "    position_limits[unit] = {}\n",
    "    for lim in lims:\n",
    "        position_limits[unit][lim] = float('inf') if (lim[1:]=='min') else float('-inf')\n",
    "\n",
    "# ===== collect positions (in meters) & leadership values per group =====\n",
    "temp_measure = list(leadership.keys())[0]   # this is only used to access general variable shapes\n",
    "\n",
    "for trial in leadership[temp_measure].keys():\n",
    "    for seg in leadership[temp_measure][trial].keys():\n",
    "        num_networks, N = leadership[temp_measure][trial][seg].shape\n",
    "        group = 'N' + str(N)\n",
    "\n",
    "        ## ----- get positions (meters) -----\n",
    "        seg_df = df[(df['trial']==trial) & (df['segment']==seg)]\n",
    "        # get node positions (absolute; in m) in the given segment\n",
    "        x_M_seg = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                               values='x_transformed').to_numpy()\n",
    "        y_M_seg = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                        values='y_transformed').to_numpy()\n",
    "        # get node positions (absolute; in m) in the 1st frame of each time window per network\n",
    "        x_M = x_M_seg[::num_frames_network, :][:num_networks,:].flatten()   # (num_networks, N) -> flattened\n",
    "        y_M = y_M_seg[::num_frames_network, :][:num_networks,:].flatten()   # (num_networks, N) -> flattened\n",
    "        # get a mask\n",
    "        temp_value = leadership[temp_measure][trial][seg].flatten()      # (num_networks, N) -> flattened\n",
    "        mask = ~np.isnan(temp_value) * ~np.isnan(x_M) * ~np.isnan(y_M)   # look for nan\n",
    "        # add positions to lists\n",
    "        positions[group]['x']['m'] += list(x_M[mask])\n",
    "        positions[group]['y']['m'] += list(y_M[mask])\n",
    "        # add to temporary lists; used to calculate position_limits\n",
    "        temp_all_x_m += list(x_M[mask])   \n",
    "        temp_all_y_m += list(y_M[mask])\n",
    "\n",
    "# ===== calculate PDF per group =====\n",
    "for group in groups:\n",
    "    # --- find outliers based on cell probability threshold ---\n",
    "    mask = get_filter(positions[group]['x']['m'], positions[group]['y']['m'],\n",
    "                      xmin=int(np.floor(np.min(temp_all_x_m))), \n",
    "                      xmax=int(np.ceil(np.max(temp_all_x_m))),\n",
    "                      ymin=int(np.floor(np.min(temp_all_y_m))), \n",
    "                      ymax=int(np.ceil(np.max(temp_all_y_m))),\n",
    "                      grid_width=grid_width['m'],\n",
    "                      threshold=prob_threshold[group])\n",
    "    # --- update xy poitions (in m) & leadership (filter outliers out) ---\n",
    "    positions[group]['x']['m'] = list(np.array(positions[group]['x']['m'])[mask])\n",
    "    positions[group]['y']['m'] = list(np.array(positions[group]['y']['m'])[mask])\n",
    "    positions['overall']['x']['m'] += positions[group]['x']['m']\n",
    "    positions['overall']['y']['m'] += positions[group]['y']['m']\n",
    "\n",
    "position_limits['m']['xmin'] = int(np.floor(np.min(positions['overall']['x']['m'])))\n",
    "position_limits['m']['xmax'] = int(np.ceil(np.max(positions['overall']['x']['m'])))\n",
    "position_limits['m']['ymin'] = int(np.floor(np.min(positions['overall']['y']['m'])))\n",
    "position_limits['m']['ymax'] = int(np.ceil(np.max(positions['overall']['y']['m'])))\n",
    "\n",
    "SD_width_x, SD_width_y = {}, {}\n",
    "for group in groups:\n",
    "    # --- recalculate PDF to get SD per group ---\n",
    "    SD_width_x[group], SD_width_y[group], _ = get_SD(positions[group]['x']['m'], positions[group]['y']['m'],\n",
    "                                                     xmin=position_limits['m']['xmin'], xmax=position_limits['m']['xmax'], \n",
    "                                                     ymin=position_limits['m']['ymin'], ymax=position_limits['m']['ymax'], \n",
    "                                                     grid_width=grid_width['m'])\n",
    "    print(f\"{group} (probability threshold = {prob_threshold[group]}) :\", end=\" \")\n",
    "    print(SD_width_x[group], SD_width_y[group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for trial in leadership[list(measure_types)[0]].keys():\n",
    "    for seg in leadership[list(measure_types)[0]][trial].keys():\n",
    "        num_networks, N = leadership[list(measure_types)[0]][trial][seg].shape\n",
    "        cond = 'N'+str(N)\n",
    "        if cond not in output.keys():\n",
    "            output[cond] = {}\n",
    "\n",
    "        ## ----- positions (meters) -----\n",
    "        seg_df = df[(df['trial']==trial) & (df['segment']==seg)]\n",
    "        # get node positions (absolute; in m) in the given segment\n",
    "        x_M_seg = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                        values='x_transformed').to_numpy()     # values='x'\n",
    "        y_M_seg = seg_df.pivot(index='frame_segment', columns='ID', \n",
    "                        values='y_transformed').to_numpy()     # values='y'\n",
    "        # get node positions (absolute; in m) in the 1st frame of each time window per network\n",
    "        x_M = x_M_seg[::num_frames_network, :][:num_networks,:]   # (num_networks, N)\n",
    "        y_M = y_M_seg[::num_frames_network, :][:num_networks,:]   # (num_networks, N)\n",
    "        x_SD, y_SD = transform_m2SD(x_M, y_M, SD_width_x[cond], SD_width_y[cond])   # (num_networks, N)\n",
    "        \n",
    "        for ped in range(N):\n",
    "            # get a mask\n",
    "            temp_value = leadership[list(measure_types)[0]][trial][seg][:,ped]     # (num_networks, N)\n",
    "            mask = ~np.isnan(temp_value) * ~np.isnan(x_SD[:,ped]) * ~np.isnan(y_SD[:,ped])   # look for nan\n",
    "\n",
    "            if ped not in output[cond].keys():\n",
    "                output[cond][ped] = {}\n",
    "                output[cond][ped]['x_m'] = list(x_M[:,ped][mask])\n",
    "                output[cond][ped]['y_m'] = list(y_M[:,ped][mask])\n",
    "                output[cond][ped]['x_SD'] = list(x_SD[:,ped][mask])\n",
    "                output[cond][ped]['y_SD'] = list(y_SD[:,ped][mask])\n",
    "                for measure_type in measure_types:\n",
    "                    output[cond][ped][measure_type] = list(leadership[measure_type][trial][seg][:,ped][mask])\n",
    "            else:\n",
    "                output[cond][ped]['x_m'] += list(x_M[:,ped][mask])\n",
    "                output[cond][ped]['y_m'] += list(y_M[:,ped][mask])\n",
    "                output[cond][ped]['x_SD'] += list(x_SD[:,ped][mask])\n",
    "                output[cond][ped]['y_SD'] += list(y_SD[:,ped][mask])\n",
    "                for measure_type in measure_types:\n",
    "                    output[cond][ped][measure_type] += list(leadership[measure_type][trial][seg][:,ped][mask])\n",
    "\n",
    "# normalize ranking\n",
    "for measure_type in measure_types:\n",
    "    if measure_type[2:] != 'rank':\n",
    "        continue\n",
    "\n",
    "    for cond in output.keys():\n",
    "        N = int(cond[1:])\n",
    "        for ped in output[cond].keys():\n",
    "            output[cond][ped][measure_type] = np.array(output[cond][ped][measure_type]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to plot all groups together\n",
    "\n",
    "cond_colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
    "cond_markers = [\"o\", \"x\", \"^\"]\n",
    "\n",
    "for measure_type in measure_types:\n",
    "    print(measure_type)\n",
    "\n",
    "    for unit in units:\n",
    "        print('--', unit)\n",
    "        for plot_type in [\"mean_only\", \"confidence_ellipse\", \"scatter\"]:    # 0: plot scatter, 1: plot confidence ellipse\n",
    "            # print('----', plot_type)\n",
    "            ver_min, ver_max = [-1.1, 1.1]\n",
    "            if plot_type==\"scatter\":\n",
    "                hor_min, hor_max = [-2, 2]\n",
    "            else:   # \"scatter\"\n",
    "                # hor_min, hor_max = [-3, 4]\n",
    "                hor_min, hor_max = [-3, 3]\n",
    "                \n",
    "            # Plot\n",
    "            _, ax = plt.subplots(figsize=(10, 6))\n",
    "            all_hor_mean, all_ver_mean = [], []     # (total_N,)\n",
    "\n",
    "            # for i, cond in enumerate(output.keys()):\n",
    "            for i, cond in enumerate(['N10', 'N16', 'N20']):\n",
    "                cond_hor_mean, cond_ver_mean = [], []   # (N,)\n",
    "\n",
    "                for ped in output[cond].keys():\n",
    "                    if plot_type==\"confidence_ellipse\":\n",
    "                        plot_confidence_ellipse(np.array(output[cond][ped]['y_'+unit]), \n",
    "                                                np.array(output[cond][ped][measure_type]), \n",
    "                                                ax, n_std=n_std, edgecolor='black',\n",
    "                                                alpha=0.2)\n",
    "                    elif plot_type==\"scatter\":\n",
    "                        ax.scatter(np.array(output[cond][ped]['y_'+unit]), \n",
    "                                    np.array(output[cond][ped][measure_type]))\n",
    "                    # calculate nanmean of y and measure_type for the given ped\n",
    "                    cond_hor_mean.append(np.nanmean(output[cond][ped]['y_'+unit]))\n",
    "                    cond_ver_mean.append(np.nanmean(output[cond][ped][measure_type]))\n",
    "                # add to lists of all datapoints\n",
    "                all_hor_mean += cond_hor_mean\n",
    "                all_ver_mean += cond_ver_mean\n",
    "                # change to numpy arrays\n",
    "                cond_hor_mean = np.array(cond_hor_mean)\n",
    "                cond_ver_mean = np.array(cond_ver_mean)\n",
    "                # plot means in the group \n",
    "                ax.scatter(cond_hor_mean, cond_ver_mean, \n",
    "                           color=cond_colors[i],\n",
    "                           marker=cond_markers[i],\n",
    "                           label=cond)\n",
    "                # print r_squared for the group\n",
    "                m, b = np.polyfit(cond_hor_mean, cond_ver_mean, 1)\n",
    "                r_squared = r2_score(cond_ver_mean, m * cond_hor_mean + b)\n",
    "                if (plot_type==\"confidence_ellipse\"):\n",
    "                    print('    ', cond, ':', f'y = {m:.2f}x {b:+.2f}, R^2 = {r_squared:.2f}')\n",
    "            \n",
    "            all_hor_mean = np.array(all_hor_mean)\n",
    "            all_ver_mean = np.array(all_ver_mean)\n",
    "\n",
    "            # obtain m (slope) and b(intercept) of linear regression line\n",
    "            m, b = np.polyfit(all_hor_mean, all_ver_mean, 1)\n",
    "            r_squared = r2_score(all_ver_mean, m * all_hor_mean + b)\n",
    "            plt.axline(xy1=(0, b), slope=m, \n",
    "                       label=f'$y = {m:.2f}x {b:+.2f}$, $R^2 = {r_squared:.2f}$',\n",
    "                       color='red')\n",
    "            if (plot_type==\"confidence_ellipse\"):\n",
    "                print('     overall :', f'y = {m:.2f}x {b:+.2f}, R^2 = {r_squared:.2f}')\n",
    "            \n",
    "            ax.set_xlim(hor_min, hor_max)\n",
    "            ax.set_ylim(ver_min, ver_max)\n",
    "            plt.xlabel('Back - Front (Mean Y axis; ' + unit + ')', fontsize=15)\n",
    "            plt.ylabel('Mean ' + measure_type + ' value', fontsize=15)\n",
    "            plt.legend()\n",
    "            # plt.title('Mean ' + measure_type + ' as a function of mean Y axis (' + unit + ') - overall', fontsize=18)\n",
    "\n",
    "            for form in ['png', 'svg']:\n",
    "                new_folder = '../output/sayles/'+form+'/individual_leadership/' + plot_type + '/' + unit + '/'\n",
    "                if not os.path.exists(new_folder):\n",
    "                    os.makedirs(new_folder)\n",
    "                plt.savefig(os.path.join(new_folder, 'individual_leadership_' + measure_type + '_overall.' + form))\n",
    "            # plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "\n",
    "    print(end='\\n')\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this to plot individual group separately\n",
    "\n",
    "# for measure_type in measure_types:\n",
    "#     print(measure_type, end=' : ')\n",
    "#     for cond in output.keys():\n",
    "#         print(cond, end=' ')\n",
    "\n",
    "#         for unit in units:\n",
    "#             for plot_type in [0,1]:    # 0: plot scatter, 1: plot confidence ellipse\n",
    "#                 # Plot\n",
    "#                 _, ax = plt.subplots(figsize=(10, 6))\n",
    "#                 ls_hor_mean, ls_ver_mean = [], []\n",
    "\n",
    "#                 ver_min, ver_max = [-1.1, 1.1]\n",
    "#                 if plot_type:\n",
    "#                     hor_min, hor_max = [-3, 4]\n",
    "#                 else:\n",
    "#                     hor_min, hor_max = [-2, 2]\n",
    "\n",
    "#                 for ped in output[cond].keys():\n",
    "#                     if plot_type:\n",
    "#                         plot_confidence_ellipse(np.array(output[cond][ped]['y_'+unit]), \n",
    "#                                                 np.array(output[cond][ped][measure_type]), \n",
    "#                                                 ax, n_std=n_std, edgecolor='black')\n",
    "#                     else:\n",
    "#                         ax.scatter(np.array(output[cond][ped]['y_'+unit]), \n",
    "#                                     np.array(output[cond][ped][measure_type]))\n",
    "\n",
    "#                     # Calculate nanmean of y and measure_type\n",
    "#                     ls_hor_mean.append(np.nanmean(output[cond][ped]['y_'+unit]))\n",
    "#                     ls_ver_mean.append(np.nanmean(output[cond][ped][measure_type]))\n",
    "\n",
    "#                 ls_hor_mean = np.array(ls_hor_mean)     # (total_N,)\n",
    "#                 ls_ver_mean = np.array(ls_ver_mean)     # (total_N,)\n",
    "#                 # obtain m (slope) and b(intercept) of linear regression line\n",
    "#                 m, b = np.polyfit(ls_hor_mean, ls_ver_mean, 1)\n",
    "#                 r_squared = r2_score(ls_ver_mean, m * ls_hor_mean + b)\n",
    "#                 plt.axline(xy1=(0, b), slope=m, \n",
    "#                         #    label=f'$y = {m:.1f}x {b:+.1f}$',\n",
    "#                         label=f'$y = {m:.1f}x {b:+.1f}$, $R^2 = {r_squared:.2f}$',\n",
    "#                         color='red')\n",
    "#                 plt.scatter(ls_hor_mean, ls_ver_mean, color='blue')\n",
    "\n",
    "#                 ax.set_xlim(hor_min, hor_max)\n",
    "#                 ax.set_ylim(ver_min, ver_max)\n",
    "#                 plt.xlabel('Back - Front (Mean Y axis; ' + unit + ')')\n",
    "#                 plt.ylabel('Mean ' + measure_type + ' value')\n",
    "#                 plt.legend()\n",
    "#                 plt.title('Mean ' + measure_type + ' as a function of mean Y axis (' + unit + ') - ' + cond)\n",
    "\n",
    "#                 for form in ['png', 'svg']:\n",
    "#                     new_folder = '../output/sayles/'+form+'/individual_leadership/'\n",
    "#                     if plot_confidence_ellipse:\n",
    "#                         new_folder += 'confidence_ellipse/'\n",
    "#                     else:\n",
    "#                         new_folder += 'scatter/'\n",
    "#                     if not os.path.exists(new_folder):\n",
    "#                         os.makedirs(new_folder)\n",
    "#                     plt.savefig(os.path.join(new_folder, 'individual_leadership_' + measure_type + '_' + cond + '_' + unit + '.' + form))\n",
    "#                 # plt.show()\n",
    "#                 plt.clf()\n",
    "#                 plt.close()\n",
    "\n",
    "#     print(end='\\n')\n",
    "    \n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "for measure_type in measure_types:\n",
    "    print(measure_type)\n",
    "\n",
    "    for unit in ['SD']:\n",
    "        print('--', unit)\n",
    "        for plot_type in [\"confidence_ellipse\", \"scatter\"]:    # 0: plot scatter, 1: plot confidence ellipse\n",
    "            # print('----', plot_type)\n",
    "\n",
    "            all_hor_mean, all_ver_mean = [], []     # (total_N,)\n",
    "\n",
    "            # for i, cond in enumerate(output.keys()):\n",
    "            for i, cond in enumerate(['N10', 'N16', 'N20']):\n",
    "                cond_hor_mean, cond_ver_mean = [], []   # (N,)\n",
    "\n",
    "                for ped in output[cond].keys():\n",
    "                    # calculate nanmean of y and measure_type for the given ped\n",
    "                    cond_hor_mean.append(np.nanmean(output[cond][ped]['y_'+unit]))\n",
    "                    cond_ver_mean.append(np.nanmean(output[cond][ped][measure_type]))\n",
    "                # add to lists of all datapoints\n",
    "                all_hor_mean += cond_hor_mean\n",
    "                all_ver_mean += cond_ver_mean\n",
    "                # change to numpy arrays\n",
    "                cond_hor_mean = np.array(cond_hor_mean)\n",
    "                cond_ver_mean = np.array(cond_ver_mean)\n",
    "\n",
    "                # # print r_squared for the group\n",
    "                # m, b = np.polyfit(cond_hor_mean, cond_ver_mean, 1)\n",
    "                # r_squared = r2_score(cond_ver_mean, m * cond_hor_mean + b)\n",
    "                # if (plot_type==\"confidence_ellipse\"):\n",
    "                #     print('    ', cond, ':', f'y = {m:.1f}x {b:+.1f}, R^2 = {r_squared:.2f}')\n",
    "\n",
    "                #add constant to predictor variables\n",
    "                cond_hor_mean = sm.add_constant(cond_hor_mean)\n",
    "                #fit linear regression model\n",
    "                model = sm.OLS(cond_ver_mean, cond_hor_mean).fit()\n",
    "                print(cond)\n",
    "                print(model.summary())\n",
    "            \n",
    "            all_hor_mean = np.array(all_hor_mean)\n",
    "            all_ver_mean = np.array(all_ver_mean)\n",
    "\n",
    "            # # obtain m (slope) and b(intercept) of linear regression line\n",
    "            # m, b = np.polyfit(all_hor_mean, all_ver_mean, 1)\n",
    "            # r_squared = r2_score(all_ver_mean, m * all_hor_mean + b)\n",
    "            # if (plot_type==\"confidence_ellipse\"):\n",
    "            #     print('     overall :', f'y = {m:.1f}x {b:+.1f}, R^2 = {r_squared:.2f}')\n",
    "\n",
    "            #add constant to predictor variables\n",
    "            all_hor_mean = sm.add_constant(all_hor_mean)\n",
    "            #fit linear regression model\n",
    "            model = sm.OLS(all_ver_mean, all_hor_mean).fit()\n",
    "            print('overall')\n",
    "            print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
